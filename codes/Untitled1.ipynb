{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"11OI2RFL48NE3uYL21tubP0B6QUcL8SAP","authorship_tag":"ABX9TyP0Z05HXVBxfkDbHdIa3jzr"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"E4ECeRBjQKlG"},"outputs":[],"source":["from sqlalchemy import TEXT\n","#import gensim\n","# from gensim import corpora\n","# from pprint import pprint\n","\n","# import pandas as pd\n","\n","# # Load the dataset\n","# df = pd.read_csv(\"/content/cleaned_no_NaN.csv\")\n","# df = df.head(1000)\n","\n","# # Initialize an empty list to store the generated sentences\n","# generated_sentences = []\n","\n","# # Iterate over each row in the dataframe\n","# for index, row in df.iterrows():\n","#     # Extract the text from the current row\n","#     text = row[\"processed_description\"]\n","    \n","#     # If the text is empty or NaN, skip to the next row\n","#     if not isinstance(text, str) or text.strip() == \"\":\n","#         continue\n","\n","#     # Tokenize the document\n","#     tokenized_docs = [text.lower().split()]\n","\n","#     # Create a dictionary from the tokenized docs\n","#     dictionary = corpora.Dictionary(tokenized_docs)\n","\n","#     # Create a corpus from the tokenized docs\n","#     corpus = [dictionary.doc2bow(doc) for doc in tokenized_docs]\n","\n","#     # Train the LDA model\n","#     lda_model = gensim.models.LdaModel(corpus=corpus, id2word=dictionary, num_topics=2, random_state=42)\n","\n","#     # Get the top 8 responses for the current document\n","#     top_topics = lda_model.top_topics(corpus, topn=8)\n","\n","#     # Concatenate the top 8 responses into a sentence and append it to the generated_sentences list\n","#     generated_sentence = \" \".join([t[0] for t in sum([top_topic[0] for top_topic in top_topics], [])])\n","#     generated_sentences.append(generated_sentence)\n","\n","# # Print the generated sentences\n","# print(generated_sentences)\n","\n","\n","import gensim\n","from gensim import corpora\n","from pprint import pprint\n","import pandas as pd\n","import re\n","\n","df = pd.read_csv(\"/content/drive/MyDrive/Copy of Latest_Clean_data.csv\")\n","df.fillna(\" \", inplace=True)\n","df = df.tail(500000)\n","keywordsen = []\n","txt = 0\n","t = 0\n","for index, row in df.iterrows():\n","    text = row[\"processed_description\"]\n","    if text == \" \":\n","      txt = row[\"processed_title\"]\n","      text = txt\n","      if txt == \" \":\n","        t = row[\"processed_bullet_points\"]\n","        text = t\n","        if text == \" \":\n","          df = df.drop(index)\n","          continue\n","\n","    # print(text)\n","    documents = [text]\n","    tokenized_docs = [doc.lower().split() for doc in documents]\n","    dictionary = corpora.Dictionary(tokenized_docs)\n","    corpus = [dictionary.doc2bow(doc) for doc in tokenized_docs]\n","    lda_model = gensim.models.LdaModel(corpus=corpus, id2word=dictionary, num_topics=2, random_state=42)\n","    topics = lda_model.print_topics(num_words=4)\n","    topic_str = \"\"\n","    for topic in topics:\n","        topic_str += str(topic[1]) + \" \"\n","   # print(\"TS\")\n","   # print(topic_str)\n","\n","\n","\n","    # Sample string\n","    sample_string = topic_str # 'The \"quick\" brown \"fox\" jumps over the \"lazy\" dog 123.'\n","\n","    # Regular expression pattern to match content within quotation marks that consists only of alphabets\n","    pattern = r'\"([A-Za-z]+)\"'\n","\n","    # Extract all matches of the pattern from the string\n","    matches = re.findall(pattern, sample_string)\n","\n","    # Print the matches\n","   # print(\"MATCH\")\n","  #  print(matches)\n","    Ksen = \"\"\n","\n","    # mainlist = []\n","    for wdr in matches:\n","      Ksen += wdr + \" \"\n","\n","    #print(\"KSEN\")\n","   # print(Ksen)\n","    keywordsen.append(Ksen)\n","    Ksen = \"\"\n","    # mainlist.append(keywordsen)\n","# print(\"KWS\")\n","# print(mainlist)"]},{"cell_type":"code","source":["df[\"KEY_SEN\"]=keywordsen\n"],"metadata":{"id":"cX8h0SFmQVq-","executionInfo":{"status":"ok","timestamp":1682265048371,"user_tz":-330,"elapsed":493,"user":{"displayName":"prakhar gupta","userId":"08883096779518728940"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["df.to_csv(\"Check_test.csv\", index=False)"],"metadata":{"id":"Z_5d1NnRQdy5","executionInfo":{"status":"ok","timestamp":1682265059972,"user_tz":-330,"elapsed":8469,"user":{"displayName":"prakhar gupta","userId":"08883096779518728940"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["df1 = pd.read_csv(\"Check_testis.csv\")\n","len(df1)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"C8Dmo-RR0CKg","executionInfo":{"status":"ok","timestamp":1682265252788,"user_tz":-330,"elapsed":5194,"user":{"displayName":"prakhar gupta","userId":"08883096779518728940"}},"outputId":"49610dc3-0934-4e5f-ff92-e095424126c0"},"execution_count":10,"outputs":[{"output_type":"execute_result","data":{"text/plain":["499674"]},"metadata":{},"execution_count":10}]}]}